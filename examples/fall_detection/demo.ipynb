{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fall detection\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/uclasystem/VQPy/blob/main/examples/fall_detection/demo.ipynb)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this example we adopt code from [Human Falling Detection and Tracking](https://github.com/GajuuzZ/Human-Falling-Detect-Tracks) to track human movement and detect action. Two models are used: [AlphaPose](https://github.com/MVIG-SJTU/AlphaPose), to get person's body keypoints from cropped image of person; and [ST-GCN](https://github.com/yysijie/st-gcn), to predict action from every 30 frames of each person's keypoints."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment setup\n",
    "\n",
    "Python3.8 is recommended to avoid compatibility issues when installing YOLOX.\n",
    "\n",
    "Please download video from [here](https://youtu.be/ctniCxIdpTY) and place it in the same directory as this notebook.\n",
    "\n",
    "You'll also need to download pre-trained models from [SPPE FastPose (AlphaPose)](https://drive.google.com/file/d/1IPfCDRwCmQDnQy94nT1V-_NVtTEi4VmU/view?usp=sharing) and [ST-GCN](https://drive.google.com/file/d/1mQQ4JHe58ylKbBqTjuKzpwN2nwKOWJ9u/view?usp=sharing) and place them in the same directory. (models from [Human Falling Detection and Tracking](https://github.com/GajuuzZ/Human-Falling-Detect-Tracks#pre-trained-models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install YOLOX\n",
    "!git clone https://github.com/Megvii-BaseDetection/YOLOX.git\n",
    "# download YOLOX pretrained model\n",
    "!wget https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_x.pth\n",
    "!cd YOLOX && pip3 install .\n",
    "# download VQPy, move vqpy/ to root directory for import\n",
    "!git clone https://github.com/uclasystem/VQPy.git\n",
    "!mv VQPy/vqpy ./\n",
    "# install VQPy's dependencies\n",
    "!pip3 install lap cython_bbox shapely\n",
    "import numpy as np\n",
    "import torch\n",
    "import vqpy\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"VQPy/examples/fall_detection/detect\")\n",
    "video_path = \"./fall.mp4\"\t# path to video\n",
    "save_folder = \"./vqpy_outputs\"\n",
    "model_dir = \"./\"\n",
    "# import AlphaPose and ST-GCN models\n",
    "from PoseEstimateLoader import SPPE_FastPose\n",
    "from ActionsEstLoader import TSSTG"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, the working directory should look like:\n",
    "\n",
    "```text\n",
    ".\n",
    "├── VQPy\n",
    "├── YOLOX\n",
    "├── fall.mp4\t# video to query on\n",
    "├── vqpy\t# make vqpy available for import\n",
    "├── yolox_x.pth\t# YOLOX model checkpoint\n",
    "├── fast_res50_256x192.pth\t# AlphaPose model checkpoint\n",
    "└── tsstg-model.pth\t#ST-GCN model checkpoint\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fall detection with VQPy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define `VObj` type for person"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interested in people's pose, we create a `Person` VObj:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person(vqpy.VObjBase):\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adapt model from pose prediction\n",
    "\n",
    "Two models are used to predict the pose a person:\n",
    "\n",
    "- AlphaPose: takes the frame and person's bounding box, returns a list of keypoints. Keypoints are mid-products to be used in ST-GCN.\n",
    "- ST-GCN: takes keypoints list of the last 30 frames, returns pose predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the two models for inference\n",
    "pose_model = SPPE_FastPose('resnet50', 224, 160, device='cuda',\n",
    "        weights_file=os.path.join(\n",
    "            os.path.abspath(model_dir), \"fast_res50_256x192.pth\"\n",
    "        )\n",
    "    )\n",
    "action_model = TSSTG(\n",
    "    weight_file=os.path.join(os.path.abspath(model_dir), \"tsstg-model.pth\")\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To store the final output, person's pose, and mid-product, list of keypoints, we create two properties in `Person` VObj.\n",
    "\n",
    "Since ST-GCN requires keypoints be stored for the last 30 frames, function that computes `keypoints` needs to be decorated with `@stateful(30)`, where `30` specifies that 30 frames of values should be saved.\n",
    "\n",
    "Adding the two properties to `Person`, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person(vqpy.VObjBase):\n",
    "    required_fields = ['class_id', 'tlbr']\n",
    "\n",
    "    @vqpy.property()\n",
    "    @vqpy.stateful(30)  # require 30 frames of \n",
    "    def keypoints(self):\n",
    "        image = self._ctx.frame\n",
    "        tlbr = self.getv('tlbr')\n",
    "        # per-frame property, tlbr could be None when tracking is lost\n",
    "        # temporary work around until we have better dependency control\n",
    "        if tlbr is None:\n",
    "            return None\n",
    "        return pose_model.predict(image, torch.tensor([tlbr]))\n",
    "\n",
    "    @vqpy.property()\n",
    "    def pose(self) -> str:\n",
    "        keypoints_list = []\n",
    "        # retrieve list of keypoints from the last 30 frames\n",
    "        # also need to deal with object lost during tracking\n",
    "        # return 'unknown' if not enough keypoints\n",
    "        for i in range(-self._track_length, 0):\n",
    "            keypoint = self.getv('keypoints', i)\n",
    "            if keypoint is not None:\n",
    "                keypoints_list.append(keypoint)\n",
    "            if len(keypoints_list) >= 30:\n",
    "                break\n",
    "        if len(keypoints_list) < 30:\n",
    "            return 'unknown'\n",
    "        # type conversion to adapt data to model input\n",
    "        pts = np.array(keypoints_list, dtype=np.float32)\n",
    "        out = action_model.predict(pts, self._ctx.frame.shape[:2])\n",
    "        action_name = action_model.class_names[out[0].argmax()]\n",
    "        return action_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The conditional statements `if tlbr is None` in L11 and iteration over `for i in range(-self._track_length, 0)` in L21 are attempting to get the most recent bounding box and keypoints. We are still working on how properties (and stateful properties) in VObjs can be accessed rather conveniently."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Query on `Person`'s pose\n",
    "\n",
    "To filter on people that are falling down, we filter on `pose` having value `\"Fall Down\"` (7 actions should be supported: `\"Standing\", \"Walking\", \"Sitting\", \"Lying Down\", \"Stand up\", \"Sit down\", \"Fall Down\"`).\n",
    "\n",
    "`filter_cons` is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_cons = {\n",
    "    '__class__': lambda x: x == Person,\n",
    "    'pose': lambda x: x == \"Fall Down\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For output, we select:\n",
    "\n",
    "- tracker id, selected with `track_id`\n",
    "- bounding box, in format of coordinate of top-left and bottom-right corner, selected with `tlbr`\n",
    "\n",
    "`select_cons` is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_cons = {\n",
    "    'track_id': None,\n",
    "    'tlbr': lambda x: str(x)\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query could be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FallDetection(vqpy.QueryBase):\n",
    "    @staticmethod\n",
    "    def setting() -> vqpy.VObjConstraint:\n",
    "        return vqpy.VObjConstraint(\n",
    "            filter_cons=filter_cons,\n",
    "            select_cons=select_cons,\n",
    "            filename='fall'\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the query\n",
    "\n",
    "With the `Person` VObj and the query defined, we can run the query, with:\n",
    "\n",
    "- `cls_name` is a tuple for mapping numerical outputs of object detector to str\n",
    "\n",
    "\te.g. `vqpy.COCO_CLASSES` here starts with `(\"person\", \"bicycle\", ...)`, meaning that we will map output `0` of object detector to COCO class `\"person\"`\n",
    "\n",
    "- `cls_type` is a dictionary that maps name of object type (in str) to VObj types defined\n",
    "\n",
    "\t`{\"person\": Person}` means we wish to map COCO class `person` to VObj type `Person`\n",
    "\n",
    "- `tasks` is a list of queries to run on the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqpy.launch(\n",
    "    cls_name=vqpy.COCO_CLASSES,\n",
    "    cls_type={\"person\": Person},\n",
    "    tasks=[FallDetection()],\n",
    "    video_path=video_path,\n",
    "    save_folder=save_folder,\n",
    "    detector_model_dir=model_dir\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expected result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result of the query will be in `{save_folder}/{video_name}_{task_name}_{detector_name}.json`, output for this example should be in `./vqpy_outputs/fall_fall_yolox.json`.\n",
    "\n",
    "One entry is created for each frame that has filter condition satisfied.\n",
    "\n",
    "e.g. The entry in frame 133:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"frame_id\": 133,\n",
    "  \"data\": [\n",
    "    { \"track_id\": 188, \"tlbr\": \"[485. 270. 796. 588.]\" }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "<img src=\"./demo.assets/fall133.png\" alt=\"with coordinate marked\" style=\"zoom: 60%;\" />"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
